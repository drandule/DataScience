{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lgwZqksRWwZ1eYDeTt-He4X-eKms18jB","timestamp":1739725571409}],"gpuType":"T4","authorship_tag":"ABX9TyOtTOI2LPw2MItd33IMU8+t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZBBJt-K5aug"},"outputs":[],"source":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import time\n","import nltk\n","from nltk.translate.bleu_score import corpus_bleu\n","import pickle\n","\n","\n","# Если ещё не скачаны нужные данные NLTK, то раскомментируйте следующую строку:\n","# nltk.download('punkt')\n","\n","\n","#########################################\n","# 1. Формирование датасета и словарей   #\n","#########################################\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, file_path, max_len=50):\n","        \"\"\"\n","        Читает файл, где каждая строка имеет формат:\n","          <английский текст><tab><украинский текст>\n","        Производит базовую токенизацию (split по пробелам) и формирует словари.\n","        \"\"\"\n","        self.src_texts = []  # английские тексты\n","        self.trg_texts = []  # украинские тексты\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if \"\\t\" in line:\n","                    src, trg = line.strip().split(\"\\t\")\n","                    self.src_texts.append(src.lower())\n","                    self.trg_texts.append(trg.lower())\n","\n","        # Токенизация (простой split по пробелам)\n","        self.src_tokenized = [src.split() for src in self.src_texts]\n","        self.trg_tokenized = [trg.split() for trg in self.trg_texts]\n","\n","        # Построение словарей (индексация). Специальные токены:\n","        # <pad>: заполнение, <sos>: начало предложения, <eos>: конец предложения, <unk>: неизвестное слово.\n","        self.src_vocab = self.build_vocab(self.src_tokenized)\n","        self.trg_vocab = self.build_vocab(self.trg_tokenized)\n","\n","        self.max_len = max_len\n","\n","    def build_vocab(self, tokenized_texts):\n","        vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n","        idx = 4\n","        for tokens in tokenized_texts:\n","            for token in tokens:\n","                if token not in vocab:\n","                    vocab[token] = idx\n","                    idx += 1\n","        return vocab\n","\n","    def __len__(self):\n","        return len(self.src_tokenized)\n","\n","    def __getitem__(self, idx):\n","        # Преобразуем исходное предложение в индексы\n","        src_tokens = self.src_tokenized[idx]\n","        src_indices = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src_tokens]\n","\n","        # Для целевого языка добавляем <sos> и <eos>\n","        trg_tokens = self.trg_tokenized[idx]\n","        trg_indices = [self.trg_vocab['<sos>']] + \\\n","                      [self.trg_vocab.get(token, self.trg_vocab['<unk>']) for token in trg_tokens] + \\\n","                      [self.trg_vocab['<eos>']]\n","\n","        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)\n","\n","\n","def collate_fn(batch):\n","    \"\"\"\n","    Формирует батч с паддингом.\n","    Возвращает:\n","      - src_batch: [B, max_src_len]\n","      - trg_batch: [B, max_trg_len]\n","      - src_lengths: список длин исходных последовательностей (без паддинга)\n","    \"\"\"\n","    src_batch, trg_batch = zip(*batch)\n","    src_lengths = [len(s) for s in src_batch]\n","    src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n","    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=0)\n","    return src_batch, trg_batch, src_lengths\n","\n","\n","#########################################\n","# 2. Архитектура нейронной сети (Seq2Seq)#\n","#    с механизмом внимания              #\n","#########################################\n","\n","# --- ЭНКОДЕР ---\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n","\n","    def forward(self, src, src_lengths):\n","        # src: [B, src_len]\n","        embedded = self.embedding(src)  # [B, src_len, emb_dim]\n","        # Упаковываем последовательности для LSTM\n","        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False)\n","        packed_outputs, (hidden, cell) = self.rnn(packed)\n","        # Восстанавливаем последовательности\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n","        # outputs: [B, src_len, hid_dim]\n","        return outputs, hidden, cell\n","\n","\n","# --- МОДУЛЬ ВНИМАНИЯ (Attention) ---\n","class Attention(nn.Module):\n","    def __init__(self, hid_dim):\n","        super(Attention, self).__init__()\n","        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n","        self.v = nn.Linear(hid_dim, 1, bias=False)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        \"\"\"\n","        hidden: [B, hid_dim] — текущее состояние декодера (последний слой)\n","        encoder_outputs: [B, src_len, hid_dim] — все выходы энкодера\n","        Возвращает: веса внимания [B, src_len]\n","        \"\"\"\n","        batch_size = encoder_outputs.size(0)\n","        src_len = encoder_outputs.size(1)\n","\n","        # Повторяем состояние декодера для каждого временного шага энкодера\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [B, src_len, hid_dim]\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [B, src_len, hid_dim]\n","        attention = self.v(energy).squeeze(2)  # [B, src_len]\n","        return torch.softmax(attention, dim=1)\n","\n","\n","# --- ДЕКОДЕР с вниманием ---\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, attention):\n","        super(Decoder, self).__init__()\n","        self.output_dim = output_dim\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        # На вход подаём объединённый вектор: embedding + context\n","        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, batch_first=True)\n","        # Для предсказания токена используем объединение: [output, context, embedded]\n","        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n","\n","    def forward(self, input, hidden, cell, encoder_outputs):\n","        # input: [B] — один токен (индексы)\n","        input = input.unsqueeze(1)  # [B, 1]\n","        embedded = self.embedding(input)  # [B, 1, emb_dim]\n","\n","        # Вычисляем веса внимания на основе последнего слоя состояния декодера\n","        a = self.attention(hidden[-1], encoder_outputs)  # [B, src_len]\n","        a = a.unsqueeze(1)  # [B, 1, src_len]\n","        # Контекст – взвешенная сумма выходов энкодера\n","        context = torch.bmm(a, encoder_outputs)  # [B, 1, hid_dim]\n","\n","        # Объединяем embedding и context\n","        rnn_input = torch.cat((embedded, context), dim=2)  # [B, 1, emb_dim+hid_dim]\n","\n","        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n","        # output: [B, 1, hid_dim]\n","        output = output.squeeze(1)  # [B, hid_dim]\n","        embedded = embedded.squeeze(1)  # [B, emb_dim]\n","        context = context.squeeze(1)  # [B, hid_dim]\n","        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))  # [B, output_dim]\n","        return prediction, hidden, cell, a.squeeze(1)\n","\n","\n","# --- Модель Seq2Seq ---\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n","        \"\"\"\n","        src: [B, src_len]\n","        trg: [B, trg_len] (с токеном <sos> на позиции 0)\n","        \"\"\"\n","        batch_size = src.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","\n","        # Получаем выходы энкодера и начальные состояния\n","        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n","\n","        # Первый токен декодера – <sos>\n","        input = trg[:, 0]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n","            outputs[:, t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs\n","\n","\n","#########################################\n","# 3. Функции обучения, перевода и оценки #\n","#########################################\n","\n","def train_epoch(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    epoch_loss = 0\n","    for src, trg, src_lengths in dataloader:\n","        src, trg = src.to(device), trg.to(device)\n","        optimizer.zero_grad()\n","        output = model(src, src_lengths, trg, teacher_forcing_ratio=0.5)\n","        # output: [B, trg_len, output_dim]\n","        output_dim = output.shape[-1]\n","        output = output[:, 1:].reshape(-1, output_dim)\n","        trg = trg[:, 1:].reshape(-1)\n","        loss = criterion(output, trg)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    return epoch_loss / len(dataloader)\n","\n","\n","def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n","    \"\"\"\n","    Перевод одного предложения.\n","    sentence: строка с английским текстом.\n","    Возвращает список токенов перевода.\n","    \"\"\"\n","    model.eval()\n","    tokens = sentence.lower().split()\n","    indices = [src_vocab.get(token, src_vocab['<unk>']) for token in tokens]\n","    src_tensor = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(0)\n","    src_len = [len(indices)]\n","\n","    with torch.no_grad():\n","        encoder_outputs, hidden, cell = model.encoder(src_tensor, src_len)\n","\n","    trg_indices = [trg_vocab['<sos>']]\n","\n","    for _ in range(max_len):\n","        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long, device=device)\n","        with torch.no_grad():\n","            output, hidden, cell, _ = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n","        pred_token = output.argmax(1).item()\n","        trg_indices.append(pred_token)\n","        if pred_token == trg_vocab['<eos>']:\n","            break\n","\n","    inv_trg_vocab = {v: k for k, v in trg_vocab.items()}\n","    translated_tokens = [inv_trg_vocab.get(idx, '<unk>') for idx in trg_indices]\n","    return translated_tokens\n","\n","\n","def evaluate_bleu(model, dataset, device, src_vocab, trg_vocab):\n","    \"\"\"\n","    Вычисляет BLEU score на всём датасете.\n","    \"\"\"\n","    references = []\n","    hypotheses = []\n","    for i in range(len(dataset)):\n","        src_sentence = dataset.src_texts[i]\n","        trg_sentence = dataset.trg_texts[i].split()\n","        translation = translate_sentence(model, src_sentence, src_vocab, trg_vocab, device)\n","        if translation[0] == '<sos>':\n","            translation = translation[1:]\n","        if '<eos>' in translation:\n","            translation = translation[:translation.index('<eos>')]\n","        references.append([trg_sentence])\n","        hypotheses.append(translation)\n","    bleu = corpus_bleu(references, hypotheses)\n","    return bleu\n","\n","\n","#########################################\n","# 4. Основной запуск (обучение, сохранение, оценка) #\n","#########################################\n","\n","def main():\n","    # Параметры\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    BATCH_SIZE = 32\n","    N_EPOCHS = 10  # Для демонстрации – можно увеличить число эпох\n","    ENC_EMB_DIM = 256\n","    DEC_EMB_DIM = 256\n","    HID_DIM = 512\n","    N_LAYERS = 1\n","\n","    # Загружаем датасет\n","    dataset = TranslationDataset(\"data/eng-ukr.txt\")\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n","\n","    INPUT_DIM = len(dataset.src_vocab)\n","    OUTPUT_DIM = len(dataset.trg_vocab)\n","\n","    # Инициализируем компоненты модели\n","    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, n_layers=N_LAYERS)\n","    attention = Attention(HID_DIM)\n","    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, n_layers=N_LAYERS, attention=attention)\n","    model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n","\n","    optimizer = optim.Adam(model.parameters())\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.trg_vocab['<pad>'])\n","\n","    print(f\"Начало обучения на {N_EPOCHS} эпох(ах)...\")\n","    for epoch in range(N_EPOCHS):\n","        start_time = time.time()\n","        train_loss = train_epoch(model, dataloader, optimizer, criterion, DEVICE)\n","        end_time = time.time()\n","        print(f\"Эпоха {epoch + 1}/{N_EPOCHS} | Потеря: {train_loss:.3f} | Время: {end_time - start_time:.2f} сек\")\n","\n","    # Сохранение модели и словарей для последующего использования\n","    torch.save(model.state_dict(), \"data/seq2seq_attention_model.pth\")\n","    with open(\"data/src_vocab.pkl\", \"wb\") as f:\n","        pickle.dump(dataset.src_vocab, f)\n","    with open(\"data/trg_vocab.pkl\", \"wb\") as f:\n","        pickle.dump(dataset.trg_vocab, f)\n","\n","    # Оценка качества перевода с помощью BLEU\n","    bleu_score = evaluate_bleu(model, dataset, DEVICE, dataset.src_vocab, dataset.trg_vocab)\n","    print(f\"BLEU score: {bleu_score * 100:.2f}\")\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4nqrSAMg8gWq","executionInfo":{"status":"ok","timestamp":1739724544650,"user_tz":-180,"elapsed":11873763,"user":{"displayName":"Железный Дровосек","userId":"04995108126363539699"}},"outputId":"7b6d6f6e-627a-4401-b591-f921a232c74c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Начало обучения на 10 эпох(ах)...\n","Эпоха 1/10 | Потеря: 3.439 | Время: 957.78 сек\n","Эпоха 2/10 | Потеря: 1.795 | Время: 956.28 сек\n","Эпоха 3/10 | Потеря: 1.283 | Время: 954.66 сек\n","Эпоха 4/10 | Потеря: 1.056 | Время: 953.43 сек\n","Эпоха 5/10 | Потеря: 0.929 | Время: 951.93 сек\n","Эпоха 6/10 | Потеря: 0.840 | Время: 953.53 сек\n","Эпоха 7/10 | Потеря: 0.768 | Время: 950.88 сек\n","Эпоха 8/10 | Потеря: 0.724 | Время: 951.69 сек\n","Эпоха 9/10 | Потеря: 0.685 | Время: 950.93 сек\n","Эпоха 10/10 | Потеря: 0.651 | Время: 950.41 сек\n","BLEU score: 68.57\n"]}]}]}